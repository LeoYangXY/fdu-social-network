from flask import Flask, render_template, request, jsonify, send_from_directory
import networkx as nx
import matplotlib
matplotlib.use('Agg')  # âœ… å…³é”®ä¿®å¤ï¼šç¦ç”¨ GUI åç«¯
import matplotlib.pyplot as plt
import community as community_louvain
import pandas as pd
from collections import Counter
import numpy as np
import base64
from io import BytesIO
import json
import os
import hashlib
import time
from datetime import datetime, timedelta
import threading
import logging
import uuid
from concurrent.futures import ThreadPoolExecutor
import gc
import random

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•ä¸ºå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
TEMPLATE_DIR = os.path.join(PROJECT_ROOT, 'templates')
STATIC_DIR = os.path.join(PROJECT_ROOT, 'static')

app = Flask(__name__, 
           template_folder=TEMPLATE_DIR,
           static_folder=STATIC_DIR)
app.config['SECRET_KEY'] = 'social_network_analyzer_key'

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ•°æ®é›†é…ç½® - ç›®å‰åªä½¿ç”¨facebookæ•°æ®é›†
DATASETS = {
    "facebook": os.path.join(PROJECT_ROOT, "data", "facebook_combined.txt"),
}

# ç¼“å­˜é…ç½®
CACHE_DIR = os.path.join(PROJECT_ROOT, "cache")
os.makedirs(CACHE_DIR, exist_ok=True)

# åˆ†æè¿›åº¦å­˜å‚¨
analysis_progress = {}
executor = ThreadPoolExecutor(max_workers=2)  # é™åˆ¶å¹¶å‘çº¿ç¨‹æ•°

class ProgressTracker:
    def __init__(self, total_steps, task_id):
        self.total_steps = total_steps
        self.current_step = 0
        self.task_id = task_id
        self.start_time = time.time()
        self.progress = 0
        self.status = "åˆå§‹åŒ–"
        self.details = ""
        self.completed = False
        self.result = None
        analysis_progress[task_id] = self
    
    def update(self, step, status, details=""):
        self.current_step = step
        self.status = status
        self.details = details
        self.progress = int((step / self.total_steps) * 100)
        analysis_progress[self.task_id] = self
        logger.info(f"Task {self.task_id}: {status} - {self.progress}%")
    
    def finish(self, result=None):
        self.progress = 100
        self.status = "å®Œæˆ"
        self.result = result
        self.completed = True
        analysis_progress[self.task_id] = self

# âœ…ã€å…³é”®ä¿®æ”¹ã€‘ï¼šç”¨ JSON æ›¿ä»£ pickle çš„ CacheManager
class CacheManager:
    def __init__(self, cache_dir=CACHE_DIR, ttl_hours=24):
        self.cache_dir = cache_dir
        self.ttl_seconds = ttl_hours * 3600
        self.lock = threading.Lock()
    
    def _get_cache_key(self, dataset_path, params):
        """ç”Ÿæˆç¼“å­˜é”®"""
        version = "v1_json"
        cache_input = f"{dataset_path}_{str(params)}_{version}"
        return hashlib.md5(cache_input.encode()).hexdigest()
    
    def _get_cache_file_path(self, cache_key):
        return os.path.join(self.cache_dir, f"{cache_key}.json")  # .json è€Œé .pkl
    
    def get(self, dataset_path, params):
        """è·å–ç¼“å­˜ç»“æœ"""
        cache_key = self._get_cache_key(dataset_path, params)
        cache_file = self._get_cache_file_path(cache_key)
        
        if not os.path.exists(cache_file):
            logger.info(f"Cache miss for {dataset_path}")
            return None
        
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        mtime = os.path.getmtime(cache_file)
        if time.time() - mtime > self.ttl_seconds:
            try:
                os.remove(cache_file)
            except:
                pass
            logger.info(f"Cache expired for {dataset_path}")
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                result = json.load(f)
            logger.info(f"Cache hit for {dataset_path}")
            return result
        except (json.JSONDecodeError, OSError, ValueError, KeyError) as e:
            logger.warning(f"Cache file corrupted: {e}. Removing it.")
            try:
                os.remove(cache_file)
            except:
                pass
            return None
    
    def put(self, dataset_path, params, result):
        """ä¿å­˜ç»“æœåˆ°ç¼“å­˜"""
        cache_key = self._get_cache_key(dataset_path, params)
        cache_file = self._get_cache_file_path(cache_key)
        
        safe_result = {
            'global_metrics': result.get('global_metrics', {}),
            'opinion_leaders': result.get('opinion_leaders', []),
            'community_info': result.get('community_info', {}),
            'visualizations': result.get('visualizations', {}),
            'analysis_time': result.get('analysis_time', datetime.now().isoformat()),
            'cached': False,
            'task_id': result.get('task_id', cache_key)
        }
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(safe_result, f, ensure_ascii=False)
            logger.info(f"Cached result for {dataset_path}")
        except Exception as e:
            logger.error(f"Cache save failed: {e}")

def run_analysis_task(task_id, dataset_path, top_k):
    """åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œåˆ†æä»»åŠ¡"""
    try:
        analyzer = SocialNetworkAnalyzer(dataset_path)
        
        if not analyzer.load_graph(analysis_progress.get(task_id)):
            return {'error': 'Failed to load graph data'}
        
        global_metrics = analyzer.compute_global_metrics(analysis_progress.get(task_id))
        opinion_leaders = analyzer.identify_opinion_leaders(top_k=top_k, progress_tracker=analysis_progress.get(task_id))
        community_info = analyzer.detect_communities(progress_tracker=analysis_progress.get(task_id))
        additional_metrics = analyzer.compute_additional_metrics(progress_tracker=analysis_progress.get(task_id))
        visualizations = analyzer.generate_visualizations(progress_tracker=analysis_progress.get(task_id))
        
        all_metrics = {**global_metrics, **additional_metrics}
        
        result = {
            'global_metrics': all_metrics,
            'opinion_leaders': opinion_leaders.to_dict(orient='records') if opinion_leaders is not None else [],
            'community_info': community_info,
            'visualizations': visualizations,
            'analysis_time': datetime.now().isoformat(),
            'cached': False,
            'task_id': task_id
        }
        
        cache_manager.put(dataset_path, {'top_k': top_k}, result)
        
        if task_id in analysis_progress:
            analysis_progress[task_id].finish(result)
        
        logger.info(f"Analysis completed for task {task_id}")
        return result
    except Exception as e:
        logger.error(f"Analysis error for task {task_id}: {str(e)}")
        if task_id in analysis_progress:
            analysis_progress[task_id].finish({'error': str(e)})
        return {'error': str(e)}

class SocialNetworkAnalyzer:
    def __init__(self, dataset_path):
        self.dataset_path = dataset_path
        self.G = None
        self.partition = None
        self.leaders_df = None
        self.community_stats = None
        
    def load_graph(self, progress_tracker=None):
        if progress_tracker:
            progress_tracker.update(1, "åŠ è½½å›¾æ•°æ®", f"ä» {self.dataset_path} åŠ è½½")
        
        logger.info(f"Loading graph from {self.dataset_path}...")
        try:
            if any(x in self.dataset_path.lower() for x in ['facebook', 'amazon', 'youtube']):
                self.G = nx.read_edgelist(self.dataset_path, nodetype=int)
            elif 'vote' in self.dataset_path.lower():
                self.G = nx.read_edgelist(self.dataset_path, nodetype=int)
            else:
                self.G = nx.read_edgelist(self.dataset_path, nodetype=str)
            
            if self.G.is_directed():
                self.G = self.G.to_undirected()
            
            logger.info(f"Graph loaded: {self.G.number_of_nodes()} nodes, {self.G.number_of_edges()} edges")
            return True
        except Exception as e:
            logger.error(f"Error loading graph: {str(e)}")
            return False
    
    def identify_opinion_leaders(self, top_k=10, progress_tracker=None):
        if progress_tracker:
            progress_tracker.update(2, "è®¡ç®—åº¦ä¸­å¿ƒæ€§", "è®¡ç®—èŠ‚ç‚¹çš„åº¦ä¸­å¿ƒæ€§")
        
        try:
            degree_cent = nx.degree_centrality(self.G)
            
            if progress_tracker:
                progress_tracker.update(3, "è®¡ç®—ä»‹æ•°ä¸­å¿ƒæ€§", "è®¡ç®—èŠ‚ç‚¹çš„ä»‹æ•°ä¸­å¿ƒæ€§")
            
            # å¯¹å¤§å›¾ä½¿ç”¨é‡‡æ ·
            sample_size = min(1000, self.G.number_of_nodes())
            betweenness_cent = nx.betweenness_centrality(self.G, k=sample_size, seed=42)
            
            if progress_tracker:
                progress_tracker.update(4, "è®¡ç®—æ¥è¿‘ä¸­å¿ƒæ€§", "è®¡ç®—èŠ‚ç‚¹çš„æ¥è¿‘ä¸­å¿ƒæ€§")
            
            closeness_cent = nx.closeness_centrality(self.G)
            
            if progress_tracker:
                progress_tracker.update(5, "è®¡ç®—ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§", "è®¡ç®—èŠ‚ç‚¹çš„ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§")
            
            # å¯¹å¤§å›¾é™åˆ¶è¿­ä»£æ¬¡æ•°
            if self.G.number_of_nodes() > 5000:
                eigenvector_cent = nx.eigenvector_centrality(self.G, max_iter=100, tol=1e-3)
            else:
                eigenvector_cent = nx.eigenvector_centrality(self.G, max_iter=200, tol=1e-4)
            
            centrality_dict = {
                'degree': degree_cent,
                'betweenness': betweenness_cent,
                'closeness': closeness_cent,
                'eigenvector': eigenvector_cent
            }
            
            leaders = {}
            scores = {}
            
            for name, cent in centrality_dict.items():
                top_nodes = sorted(cent.items(), key=lambda x: x[1], reverse=True)[:top_k]
                leaders[name] = [node for node, score in top_nodes]
                scores[name] = [score for node, score in top_nodes]
            
            self.leaders_df = pd.DataFrame({
                'node': leaders['degree'],
                'degree_score': scores['degree'],
                'betweenness_score': scores['betweenness'],
                'closeness_score': scores['closeness'],
                'eigenvector_score': scores['eigenvector']
            })
            
            if progress_tracker:
                progress_tracker.update(6, "æ„è§é¢†è¢–è¯†åˆ«å®Œæˆ", f"å·²è¯†åˆ«å‰{top_k}åæ„è§é¢†è¢–")
            
            return self.leaders_df
        except Exception as e:
            logger.error(f"Error computing opinion leaders: {str(e)}")
            return None
    
    def compute_global_metrics(self, progress_tracker=None):
        if progress_tracker:
            progress_tracker.update(7, "è®¡ç®—å…¨å±€ç½‘ç»œæŒ‡æ ‡", "åˆ†æç½‘ç»œå¯†åº¦ã€èšç±»ç³»æ•°ç­‰")
        
        try:
            metrics = {
                'nodes': self.G.number_of_nodes(),
                'edges': self.G.number_of_edges(),
                'density': round(nx.density(self.G), 6),
                'average_clustering': round(nx.average_clustering(self.G), 6),
                'average_degree': round(2 * self.G.number_of_edges() / self.G.number_of_nodes(), 2),
                'transitivity': round(nx.transitivity(self.G), 6),
                'assortativity': round(nx.degree_assortativity_coefficient(self.G), 6)
            }
            
            if progress_tracker:
                progress_tracker.update(8, "åˆ†æè¿é€šæ€§", "æ£€æŸ¥ç½‘ç»œè¿é€šåˆ†é‡")
            
            components = list(nx.connected_components(self.G))
            metrics['connected_components'] = len(components)
            metrics['largest_component_size'] = max(len(c) for c in components) if components else 0
            
            if progress_tracker:
                progress_tracker.update(9, "è®¡ç®—è·¯å¾„é•¿åº¦", "è®¡ç®—ç›´å¾„ã€åŠå¾„ç­‰è·¯å¾„æŒ‡æ ‡")
            
            if nx.is_connected(self.G):
                try:
                    if self.G.number_of_nodes() > 5000:
                        # å¯¹å¤§å›¾ä½¿ç”¨è¿‘ä¼¼ç®—æ³•
                        metrics['diameter'] = "N/A (approx too expensive)"
                        metrics['radius'] = "N/A (approx too expensive)"
                        metrics['avg_shortest_path'] = "N/A (approx too expensive)"
                    else:
                        metrics['diameter'] = nx.diameter(self.G)
                        metrics['radius'] = nx.radius(self.G)
                        metrics['avg_shortest_path'] = round(nx.average_shortest_path_length(self.G), 4)
                except nx.NetworkXNoPath:
                    metrics['diameter'] = "N/A (disconnected components)"
                    metrics['radius'] = "N/A (disconnected components)"
                    metrics['avg_shortest_path'] = "N/A (disconnected components)"
                except Exception as e:
                    logger.warning(f"Could not compute path metrics: {e}")
                    metrics['diameter'] = "N/A (computation too expensive)"
                    metrics['radius'] = "N/A (computation too expensive)"
                    metrics['avg_shortest_path'] = "N/A (computation too expensive)"
            else:
                metrics['diameter'] = "N/A (disconnected)"
                metrics['radius'] = "N/A (disconnected)"
                metrics['avg_shortest_path'] = "N/A (disconnected)"
            
            return metrics
        except Exception as e:
            logger.error(f"Error computing global metrics: {str(e)}")
            return {}
    
    def detect_communities(self, progress_tracker=None):
        if progress_tracker:
            progress_tracker.update(10, "è¿è¡Œç¤¾åŒºæ£€æµ‹", "ä½¿ç”¨Louvainç®—æ³•æ£€æµ‹ç¤¾åŒºç»“æ„")
        
        logger.info("Running community detection...")
        try:
            self.partition = community_louvain.best_partition(self.G, random_state=42, resolution=1.0)
            modularity = community_louvain.modularity(self.partition, self.G)
            
            community_stats = Counter(self.partition.values())
            num_communities = len(community_stats)
            
            community_details = []
            for comm_id in range(num_communities):
                nodes_in_comm = [node for node, comm in self.partition.items() if comm == comm_id]
                subgraph = self.G.subgraph(nodes_in_comm)
                details = {
                    'id': comm_id,
                    'size': len(nodes_in_comm),
                    'internal_edges': subgraph.number_of_edges(),
                    'density': round(nx.density(subgraph), 6) if len(nodes_in_comm) > 1 else 0
                }
                community_details.append(details)
            
            self.community_stats = {
                'partition': self.partition,
                'modularity': round(modularity, 6),
                'num_communities': num_communities,
                'community_sizes': dict(community_stats),
                'details': community_details
            }
            
            if progress_tracker:
                progress_tracker.update(11, "ç¤¾åŒºæ£€æµ‹å®Œæˆ", f"å‘ç°{num_communities}ä¸ªç¤¾åŒºï¼Œæ¨¡å—åº¦:{modularity:.4f}")
            
            return self.community_stats
        except Exception as e:
            logger.error(f"Error detecting communities: {str(e)}")
            return {}
    
    def compute_additional_metrics(self, progress_tracker=None):
        if progress_tracker:
            progress_tracker.update(12, "è®¡ç®—é¢å¤–æŒ‡æ ‡", "è®¡ç®—åº¦åˆ†å¸ƒã€HæŒ‡æ•°ç­‰")
        
        try:
            additional_metrics = {}
            
            degrees = [d for n, d in self.G.degree()]
            additional_metrics['max_degree'] = max(degrees) if degrees else 0
            additional_metrics['min_degree'] = min(degrees) if degrees else 0
            additional_metrics['avg_degree'] = sum(degrees) / len(degrees) if degrees else 0
            
            degree_counts = Counter(degrees)
            additional_metrics['degree_distribution'] = dict(degree_counts)
            
            h_index = 0
            sorted_degrees = sorted(degrees, reverse=True)
            for i, deg in enumerate(sorted_degrees):
                if deg >= i + 1:
                    h_index = i + 1
                else:
                    break
            additional_metrics['h_index'] = h_index
            
            return additional_metrics
        except Exception as e:
            logger.error(f"Error computing additional metrics: {str(e)}")
            return {}
        
    def generate_visualizations(self, progress_tracker=None):
        try:
            visualizations = {}
            n_nodes = self.G.number_of_nodes()
            
            # === 1. åº¦åˆ†å¸ƒå›¾ï¼ˆå§‹ç»ˆç”Ÿæˆï¼Œå¾ˆå¿«ï¼‰===
            if progress_tracker:
                progress_tracker.update(13, "ç”Ÿæˆåº¦åˆ†å¸ƒå›¾", "å¿«é€Ÿç»˜åˆ¶åº¦åˆ†å¸ƒ")

            fig, ax = plt.subplots(figsize=(6, 4))
            degrees = [d for n, d in self.G.degree()]
            max_bins = min(30, len(set(degrees)))
            ax.hist(degrees, bins=max_bins, color='skyblue', alpha=0.8)
            ax.set_xlabel("Degree")
            ax.set_ylabel("Count")
            ax.set_title("Degree Distribution")
            visualizations['degree_dist'] = self.fig_to_base64(fig)
            plt.close(fig)

            # === 2. ç¤¾åŒºè§„æ¨¡åˆ†å¸ƒï¼ˆå¦‚æœæ£€æµ‹äº†ç¤¾åŒºï¼‰===
            if hasattr(self, 'community_stats') and self.community_stats:
                if progress_tracker:
                    progress_tracker.update(14, "ç”Ÿæˆç¤¾åŒºè§„æ¨¡åˆ†å¸ƒ", "å¿«é€Ÿç»˜åˆ¶ç¤¾åŒºå¤§å°")
                sizes = list(self.community_stats['community_sizes'].values())
                fig, ax = plt.subplots(figsize=(6, 4))
                max_bins = min(15, len(sizes))
                ax.hist(sizes, bins=max_bins, color='salmon', alpha=0.8)
                ax.set_xlabel("Community Size")
                ax.set_ylabel("Count")
                ax.set_title("Community Size Dist")
                visualizations['community_size_dist'] = self.fig_to_base64(fig)
                plt.close(fig)

            # === 3. ä¸­å¿ƒæ€§å¯¹æ¯”å›¾ï¼ˆå¦‚æœæœ‰æ„è§é¢†è¢–ï¼‰===
            if self.leaders_df is not None:
                if progress_tracker:
                    progress_tracker.update(15, "ç”Ÿæˆä¸­å¿ƒæ€§å¯¹æ¯”", "å¿«é€Ÿç»˜åˆ¶TopèŠ‚ç‚¹æŒ‡æ ‡")
                top_n = min(8, len(self.leaders_df))
                fig, ax = plt.subplots(figsize=(7, 4))
                x = range(top_n)
                for col in ['degree_score', 'betweenness_score']:
                    if col in self.leaders_df.columns:
                        ax.plot(x, self.leaders_df[col].values[:top_n], label=col.split('_')[0], marker='o')
                ax.set_title("Top Nodes: Degree vs Betweenness")
                ax.legend()
                visualizations['centrality_comparison'] = self.fig_to_base64(fig)
                plt.close(fig)

            # === 4. æ„è§é¢†è¢–ä¸ç¤¾åŒºç»“æ„å¯è§†åŒ–ï¼ˆå®Œæ•´å›¾ï¼Œç¤¾åŒºæŒ‰é¢œè‰²åŒºåˆ†ï¼Œä¼˜åŒ–å¸ƒå±€ï¼‰===
            if progress_tracker:
                progress_tracker.update(16, "ç”Ÿæˆæ„è§é¢†è¢–ç¤¾åŒºå›¾", f"å¤„ç†{n_nodes}ä¸ªèŠ‚ç‚¹")

            # ä½¿ç”¨åŸå§‹å›¾ï¼ˆä¸é‡‡æ ·ï¼‰
            subG = self.G
            sub_partition = self.partition if hasattr(self, 'partition') and self.partition else {}

            # ä½¿ç”¨kamada_kawai_layoutå¸ƒå±€ï¼Œä½¿ç¤¾åŒºç»“æ„æ›´æ˜æ˜¾
            if n_nodes <= 5000:
                # ä½¿ç”¨ Kamada-Kawai å¸ƒå±€ï¼Œèƒ½æ›´å¥½ä¿æŒç¤¾åŒºç»“æ„
                # ç§»é™¤seedå‚æ•°ä»¥å…¼å®¹ä½ç‰ˆæœ¬NetworkX
                pos = nx.kamada_kawai_layout(subG)
            else:
                # å¯¹è¶…å¤§å›¾ä½¿ç”¨ spring_layout + æ›´å¤šè¿­ä»£
                pos = nx.spring_layout(subG, k=0.8, iterations=50, seed=42)

            # åˆ›å»ºæ„è§é¢†è¢–ä¸ç¤¾åŒºç»“æ„ç»“åˆå›¾
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # ç»˜åˆ¶ç¤¾åŒºèŠ‚ç‚¹ï¼ˆæ ¹æ®ç¤¾åŒºç€è‰²ï¼‰
            if sub_partition:
                # è·å–æ‰€æœ‰ç¤¾åŒºIDå¹¶æ’åº
                comm_ids = sorted(list(set(sub_partition.values())))
                
                # ä¸ºæ¯ä¸ªç¤¾åŒºåˆ†é…ä¸€ä¸ªé¢œè‰²
                colors = plt.cm.tab20(np.linspace(0, 1, len(comm_ids)))
                
                # åˆ›å»ºé¢œè‰²æ˜ å°„
                color_map = {comm_id: colors[i] for i, comm_id in enumerate(comm_ids)}
                node_colors = [color_map[sub_partition[n]] if n in sub_partition else 'lightgray' for n in subG.nodes()]
                
                # ç»˜åˆ¶æ™®é€šèŠ‚ç‚¹ - æŒ‰ç¤¾åŒºç€è‰²
                normal_nodes = [n for n in subG.nodes() if self.leaders_df is None or n not in self.leaders_df['node'].tolist()]
                if normal_nodes:
                    # è·å–æ¯ä¸ªèŠ‚ç‚¹å¯¹åº”çš„ç¤¾åŒºID
                    normal_node_colors = [color_map[sub_partition[n]] if n in sub_partition else 'lightgray' for n in normal_nodes]
                    nx.draw_networkx_nodes(
                        subG, pos,
                        nodelist=normal_nodes,
                        node_color=normal_node_colors,      # âœ… ä½¿ç”¨ç¤¾åŒºé¢œè‰²
                        node_size=10,
                        alpha=0.6
                    )
            else:
                # å¦‚æœæ²¡æœ‰ç¤¾åŒºä¿¡æ¯ï¼Œæ‰ç”¨ç»Ÿä¸€ç°è‰²
                nx.draw_networkx_nodes(subG, pos, node_size=10, alpha=0.6, node_color='lightgray')
            
            # ç»˜åˆ¶æ„è§é¢†è¢–ï¼ˆè¦†ç›–åœ¨ä¸Šé¢ï¼Œæ ‡çº¢æ ‡å¤§ï¼‰
            if self.leaders_df is not None:
                leader_nodes = [n for n in self.leaders_df['node'].tolist() if n in subG]
                if leader_nodes:
                    # ç»˜åˆ¶æ„è§é¢†è¢–èŠ‚ç‚¹ï¼ˆçº¢è‰²ï¼Œæ›´å¤§ï¼‰
                    nx.draw_networkx_nodes(subG, pos, nodelist=leader_nodes,
                                         node_color='red',
                                         node_size=100, 
                                         edgecolors='white', 
                                         linewidths=2)
            
            # ç»˜åˆ¶è¾¹ï¼ˆåªç»˜åˆ¶éƒ¨åˆ†è¾¹ä»¥é¿å…é®æŒ¡ï¼‰
            edges = list(subG.edges())
            if len(edges) > 1000:
                edges = random.sample(edges, int(len(edges) * 0.1))
            
            nx.draw_networkx_edges(subG, pos, edgelist=edges, alpha=0.1, width=0.2)
            
            plt.axis('off')
            plt.title(f"Opinion Leaders (Red) + Communities - {len(subG.nodes())} nodes", fontsize=14, fontweight='bold')
            
            # æ·»åŠ å›¾ä¾‹
            from matplotlib.patches import Patch
            legend_elements = [
                Patch(facecolor='lightgray', label='Regular Nodes'),
                Patch(facecolor='red', label='Opinion Leaders')
            ]
            ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1, 1))
            
            visualizations['opinion_leaders_communities'] = self.fig_to_base64(fig)
            plt.close(fig)

            # === 5. ç½‘ç»œç»“æ„å›¾ï¼ˆä»…å¯¹è¾ƒå°çš„å›¾ï¼‰===
            if n_nodes <= 1000:
                if progress_tracker:
                    progress_tracker.update(17, "ç”Ÿæˆç½‘ç»œç»“æ„å›¾", f"èŠ‚ç‚¹æ•°: {n_nodes}")

                pos = nx.spring_layout(self.G, k=1, iterations=20, seed=42)

                fig, ax = plt.subplots(figsize=(6, 6))
                nx.draw_networkx_nodes(self.G, pos, node_size=5, alpha=0.6, node_color='steelblue')
                nx.draw_networkx_edges(self.G, pos, alpha=0.1, width=0.2)
                plt.axis('off')
                plt.title("Network Structure", fontsize=10)
                visualizations['network_structure'] = self.fig_to_base64(fig)
                plt.close(fig)

            if progress_tracker:
                progress_tracker.update(18, "å¯è§†åŒ–å®Œæˆ", "ç»˜å›¾ç»“æŸ")

            return visualizations

        except Exception as e:
            logger.error(f"Visualization error: {str(e)}")
            return {}
    
    def fig_to_base64(self, fig):
        buf = BytesIO()
        fig.savefig(buf, format='png', dpi=100, bbox_inches='tight')
        buf.seek(0)
        img_str = base64.b64encode(buf.getvalue()).decode('utf-8')
        buf.close()
        return img_str

# åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
cache_manager = CacheManager(ttl_hours=24)

@app.route('/')
def index():
    try:
        return render_template('index.html')
    except Exception as e:
        logger.error(f"Template not found: {e}")
        return f'''
        <html>
        <head><title>Social Network Analysis Platform</title></head>
        <body>
            <h1>ğŸŒ ç¤¾äº¤ç½‘ç»œåˆ†æå¹³å°</h1>
            <p>åç«¯APIæœåŠ¡æ­£åœ¨è¿è¡Œ</p>
            <p>APIç«¯ç‚¹:</p>
            <ul>
                <li><a href="/api/status">/api/status</a> - ç³»ç»ŸçŠ¶æ€</li>
                <li><a href="/api/datasets">/api/datasets</a> - å¯ç”¨æ•°æ®é›†</li>
            </ul>
            <p>è¯·ç¡®ä¿å‰ç«¯æ–‡ä»¶å­˜åœ¨äº templates/index.html</p>
            <p>é”™è¯¯è¯¦æƒ…: {str(e)}</p>
        </body>
        </html>
        '''

@app.route('/api/datasets')
def get_datasets():
    return jsonify(list(DATASETS.keys()))

@app.route('/api/analyze', methods=['POST'])
def analyze():
    try:
        data = request.json
        dataset_name = data.get('dataset', 'facebook')
        top_k = data.get('top_k', 10)
        use_cache = data.get('use_cache', True)
        
        if dataset_name not in DATASETS:
            return jsonify({'error': 'Invalid dataset'}), 400
        
        dataset_path = DATASETS[dataset_name]
        
        if use_cache:
            cached_result = cache_manager.get(dataset_path, {'top_k': top_k})
            if cached_result:
                # âœ… å³ä½¿ç¼“å­˜å‘½ä¸­ï¼Œä¹Ÿåˆ›å»ºä¸€ä¸ª"å·²å®Œæˆ"çš„ä»»åŠ¡è®°å½•
                task_id = str(uuid.uuid4())
                progress_tracker = ProgressTracker(18, task_id)
                progress_tracker.finish(result=cached_result)
                
                logger.info(f"Returning cached result as completed task for {dataset_name}")
                return jsonify({
                    'task_id': task_id,
                    'status': 'completed',
                    'cached': True,
                    'message': 'Analysis retrieved from cache.'
                })
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼šå¯åŠ¨æ–°ä»»åŠ¡
        task_id = str(uuid.uuid4())
        progress_tracker = ProgressTracker(18, task_id)
        executor.submit(run_analysis_task, task_id, dataset_path, top_k)
        
        return jsonify({
            'task_id': task_id,
            'status': 'started',
            'message': 'Analysis started. Use /api/progress/<task_id> to check progress.'
        })
    
    except Exception as e:
        logger.error(f"Analysis error: {str(e)}")
        return jsonify({'error': f'Analysis failed: {str(e)}'}), 500
    
@app.route('/api/progress/<task_id>')
def get_progress(task_id):
    if task_id in analysis_progress:
        progress_obj = analysis_progress[task_id]
        response_data = {
            'progress': progress_obj.progress,
            'status': progress_obj.status,
            'details': progress_obj.details,
            'current_step': progress_obj.current_step,
            'total_steps': progress_obj.total_steps,
            'completed': progress_obj.completed
        }
        
        if progress_obj.completed and progress_obj.result:
            response_data['result'] = progress_obj.result
        
        return jsonify(response_data)
    else:
        return jsonify({
            'error': 'Task not found',
            'progress': 0,
            'status': 'Unknown',
            'details': '',
            'completed': False
        }), 404

@app.route('/api/cache/clear', methods=['POST'])
def clear_cache():
    try:
        import shutil
        shutil.rmtree(CACHE_DIR)
        os.makedirs(CACHE_DIR, exist_ok=True)
        return jsonify({'message': 'Cache cleared successfully'})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/status')
def get_status():
    cache_files = len([f for f in os.listdir(CACHE_DIR) if f.endswith('.json')])
    active_tasks = len(analysis_progress)
    return jsonify({
        'status': 'running',
        'cache_entries': cache_files,
        'datasets_available': list(DATASETS.keys()),
        'active_analysis_tasks': active_tasks,
        'server_time': datetime.now().isoformat()
    })

if __name__ == '__main__':
    print("åç«¯APIæœåŠ¡å¯åŠ¨ä¸­... è¯·è®¿é—® http://localhost:5000 æŸ¥çœ‹å‰ç«¯æˆ– http://localhost:5000/api/status æŸ¥çœ‹APIçŠ¶æ€")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
    print(f"æ¨¡æ¿ç›®å½•: {TEMPLATE_DIR}")
    print(f"é™æ€ç›®å½•: {STATIC_DIR}")
    app.run(debug=False, host='0.0.0.0', port=5000)